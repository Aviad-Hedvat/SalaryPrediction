{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VAlNZH3NJWZ9"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, StringIndexer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# define plt & sns settings\n",
    "sns.set_theme()\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "plt.rcParams[\"axes.labelsize\"] = 20\n",
    "plt.rcParams[\"xtick.labelsize\"] = 20\n",
    "plt.rcParams[\"ytick.labelsize\"] = 20\n",
    "plt.rcParams[\"legend.fontsize\"] = 20\n",
    "plt.rcParams[\"legend.markerscale\"] = 1.5\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "plt.rcParams[\"legend.title_fontsize\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sdtk3UztJ6zN"
   },
   "outputs": [],
   "source": [
    "TARGET_COL = 'ANNUAL_GROSS'\n",
    "\n",
    "def fill_missing_values_with_mean(df):\n",
    "    for column in df.columns:\n",
    "        has_missing_values = df.filter(col(column).isNull()).count() > 0\n",
    "        if has_missing_values:\n",
    "            mean_value = df.select(mean(col(column))).collect()[0][0]\n",
    "            tmp = df.na.fill({column : mean_value})\n",
    "            df = tmp\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_low_correlation_cols(df):\n",
    "    cols_to_drop = ['CrdR', 'Press_per', 'Clr', 'Dribble_Succ_per', 'Rec_per', 'Season', 'Current_Club', 'League']\n",
    "    for column in cols_to_drop:\n",
    "        tmp = df.drop(column)\n",
    "        df = tmp\n",
    "\n",
    "    return df\n",
    "\n",
    "def Normalize(df):\n",
    "\n",
    "    numeric_cols = [f.name for f in df.schema.fields if not isinstance(f.dataType, StringType)]\n",
    "    copy = df\n",
    "\n",
    "    # UDF for converting column type from vector to double type\n",
    "    unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
    "\n",
    "    for column in numeric_cols:\n",
    "\n",
    "        if column == TARGET_COL:\n",
    "            continue\n",
    "\n",
    "        # VectorAssembler Transformation - Converting column to vector type\n",
    "        assembler = VectorAssembler(inputCols=[column], outputCol=column+'_Vect')\n",
    "\n",
    "        # MinMaxScaler Transformation\n",
    "        mmScaler = MinMaxScaler(inputCol=column+\"_Vect\", outputCol=column+\"_Scaled\")\n",
    "\n",
    "        # Pipeline of VectorAssembler and MinMaxScaler\n",
    "        pipeline = Pipeline(stages=[assembler, mmScaler])\n",
    "\n",
    "        # Fitting pipeline on dataframe and drop original column\n",
    "        tmp = pipeline.fit(copy).transform(copy).withColumn(column+\"_Scaled\", unlist(column+\"_Scaled\")).drop(column+\"_Vect\").drop(column)\n",
    "        copy = tmp\n",
    "\n",
    "\n",
    "    tmp = copy\n",
    "\n",
    "    # Rename each transformed column to its original name\n",
    "    for column in copy.columns:\n",
    "        if column.endswith('_Scaled'):\n",
    "            tmp = tmp.withColumnRenamed(column, column[:len(column)-7])\n",
    "\n",
    "\n",
    "    return tmp\n",
    "\n",
    "def Encoding(df):\n",
    "\n",
    "    str_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
    "\n",
    "    copy = df\n",
    "\n",
    "    # # define a UDF for LENGTH column to get the first char\n",
    "    # year_only = udf(lambda x: int(x[0]))\n",
    "\n",
    "    # convert every string column into int column\n",
    "    for column in str_cols:\n",
    "        indexer = StringIndexer(inputCol=column, outputCol=column+'_numeric')\n",
    "        copy = indexer.fit(copy).transform(copy).drop(column)\n",
    "\n",
    "    # # apply year_only function on LENGTH column\n",
    "    # copy = copy.withColumn('LENGTH_numeric', year_only(copy.LENGTH)).drop('LENGTH')\n",
    "\n",
    "    # Rename each transformed column to its original name\n",
    "    for column in copy.columns:\n",
    "        if column.endswith('_numeric'):\n",
    "            copy = copy.withColumnRenamed(column, column[:len(column)-8])\n",
    "\n",
    "\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iB_tpa9PJYIw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/14 17:34:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"FIFA_Clusters\").getOrCreate()\n",
    "\n",
    "url_18_19 = 'https://gist.githubusercontent.com/RonBless/a6aa9cd0570a09f6718115b20ec4b590/raw/ff7915725d5a7165c76b359fcff93691fdacf282/18_19.csv'\n",
    "url_19_20 = 'https://gist.githubusercontent.com/RonBless/a6aa9cd0570a09f6718115b20ec4b590/raw/ff7915725d5a7165c76b359fcff93691fdacf282/19_20.csv'\n",
    "url_20_21 = 'https://gist.githubusercontent.com/RonBless/a6aa9cd0570a09f6718115b20ec4b590/raw/ff7915725d5a7165c76b359fcff93691fdacf282/20_21.csv'\n",
    "\n",
    "spark.sparkContext.addFile(url_18_19)\n",
    "spark.sparkContext.addFile(url_19_20)\n",
    "spark.sparkContext.addFile(url_20_21)\n",
    "\n",
    "df18 = spark.read.csv(\"file://\"+SparkFiles.get(\"18_19.csv\"), header=True, inferSchema= True)\n",
    "df19 = spark.read.csv(\"file://\"+SparkFiles.get(\"19_20.csv\"), header=True, inferSchema= True)\n",
    "df20 = spark.read.csv(\"file://\"+SparkFiles.get(\"20_21.csv\"), header=True, inferSchema= True)\n",
    "\n",
    "df18_names = df18.select(\"Player\")\n",
    "df19_names = df19.select(\"Player\")\n",
    "df20_names = df20.select(\"Player\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Cl1z_CPyJcAP"
   },
   "outputs": [],
   "source": [
    "# Find names that appear in all three DataFrames\n",
    "common_names = df18_names.intersect(df19_names).intersect(df20_names)\n",
    "\n",
    "# Filter each DataFrame to keep only the common names\n",
    "df18_filtered = df18.join(common_names, on=\"Player\", how=\"inner\")\n",
    "df19_filtered = df19.join(common_names, on=\"Player\", how=\"inner\")\n",
    "df20_filtered = df20.join(common_names, on=\"Player\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZmKcykUPJtjT"
   },
   "outputs": [],
   "source": [
    "df18_filtered_sorted = df18_filtered.sort(df18_filtered.Player.asc())\n",
    "df19_filtered_sorted = df19_filtered.sort(df19_filtered.Player.asc())\n",
    "df20_filtered_sorted = df20_filtered.sort(df20_filtered.Player.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GnJdbzzFJ8yB"
   },
   "outputs": [],
   "source": [
    "df18 = fill_missing_values_with_mean(df18_filtered_sorted)\n",
    "df19 = fill_missing_values_with_mean(df19_filtered_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "L3u4OJa1KCru"
   },
   "outputs": [],
   "source": [
    "df18 = drop_low_correlation_cols(df18)\n",
    "df19 = drop_low_correlation_cols(df19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6qR8-9swKKFY"
   },
   "outputs": [],
   "source": [
    "df18_norm = Normalize(df18)\n",
    "df19_norm = Normalize(df19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IM65J9rQKPc8"
   },
   "outputs": [],
   "source": [
    "df18_norm_enc = Encoding(df18_norm)\n",
    "df19_norm_enc = Encoding(df19_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SPjTorkHKTVJ"
   },
   "outputs": [],
   "source": [
    "(train18, test18) = df18_norm_enc.randomSplit([0.8, 0.2], seed=42)\n",
    "(train19, test19) = df19_norm_enc.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "c4jv7eF-KXI2"
   },
   "outputs": [],
   "source": [
    "features18 = train18.columns\n",
    "features18.remove(TARGET_COL)\n",
    "\n",
    "features19 = train19.columns\n",
    "features19.remove(TARGET_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "V5DW9TXGKas4"
   },
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = features18, outputCol = 'features')\n",
    "vdf18 = vectorAssembler.transform(train18)\n",
    "vdf18 = vdf18.select(['features', TARGET_COL])\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = features19, outputCol = 'features')\n",
    "vdf19 = vectorAssembler.transform(train19)\n",
    "vdf19 = vdf18.select(['features', TARGET_COL])\n",
    "\n",
    "dataset = vdf18.union(vdf19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qrKT06ZJKeAI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/14 17:35:59 WARN DAGScheduler: Broadcasting large task binary with size 1395.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 1859:>                                                       (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/14 17:36:20 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/08/14 17:36:20 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/14 17:36:23 WARN DAGScheduler: Broadcasting large task binary with size 1399.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-9052.491394041243,213277.50167201037,-188591.46362888237,105538.32862182349,31879.35020020141,3935.461870670422,-89792.63163089102,-13460.90275935595,0.0,-32920.009970435436,66220.73379350257,-52099.96157120458,-6603.551847789801,23844.08675588463,-6992.044846572539,-14543.591616654492,-333926.7628543401,91754.48733184997,433672.78103456896,-467440.78493532125,174610.24961366595,11390.661160907051,30931.01538517174,63566075.36133205,44039.702848446184,-8166.337968881266,3414.321679282163,-1340.713851913851,2503.770884701617,-3.432966906658719,-790.0204687753464,-790.0204687753464]\n",
      "Intercept: 61339.806921695956\n",
      "numIterations: 100\n",
      "objectiveHistory: [0.5, 0.4048562195127415, 0.22307727987000184, 0.1359720748766768, 0.08540280616408918, 0.05062320928443028, 0.03022671388784658, 0.01948185076758977, 0.01429026824274292, 0.01173855335915408, 0.010047811162743805, 0.008082459723451767, 0.005292773616089741, 0.004604963212488235, 0.003893154131690176, 0.002074339750664723, 0.0017621958853180917, 0.0010832826825404123, 0.0010161333140393293, 0.0006920945496435222, 0.0006722391334715532, 0.0005351827715838188, 0.0004919215511940858, 0.000418156282254073, 0.00037076191475670566, 0.00030460768070956123, 0.00025900465779818, 0.00020812006092546455, 0.0001790231142406477, 0.00015944577164727825, 0.00013876186107138398, 0.00011916418035067064, 0.00010904713666168341, 9.358557963482746e-05, 8.186900096395558e-05, 6.211948933312573e-05, 5.187796127662754e-05, 4.33009605550704e-05, 3.133442889388808e-05, 2.8342553214207575e-05, 2.5348104749824956e-05, 2.344007774271208e-05, 2.198150976433957e-05, 2.1099866194283972e-05, 2.0197312744932887e-05, 1.9758949958837954e-05, 1.9472311995664278e-05, 1.821078440969783e-05, 1.7513290225929064e-05, 1.6025712351964996e-05, 1.4278555057786696e-05, 1.4113651829295197e-05, 1.3525883497511013e-05, 1.3000397949464837e-05, 1.2647484765643559e-05, 1.2499069936521684e-05, 1.242138427420677e-05, 1.2134608907847894e-05, 1.198783353419592e-05, 1.1872961134772386e-05, 1.165075742062859e-05, 1.1296201977414635e-05, 1.1188143701870213e-05, 1.0965229529169704e-05, 1.080122226485155e-05, 1.0698530205030423e-05, 1.0614065033953392e-05, 1.0494016201945535e-05, 1.0379208647335705e-05, 1.0284459398566126e-05, 1.0168272797474702e-05, 1.006058455888409e-05, 1.0010322529756867e-05, 9.891902079062858e-06, 9.824997491692492e-06, 9.756534911576653e-06, 9.681190146123913e-06, 9.617362962758246e-06, 9.544314842138379e-06, 9.468704159881454e-06, 9.398441398539563e-06, 9.31684080041427e-06, 9.257399926863163e-06, 9.184197114718444e-06, 9.123255156680964e-06, 9.054152740216395e-06, 8.966143174759768e-06, 8.894796756714887e-06, 8.827227804587194e-06, 8.751631714153789e-06, 8.67251557117567e-06, 8.588977219796421e-06, 8.516277151133406e-06, 8.42283648981287e-06, 8.34420545916117e-06, 8.250904336280161e-06, 8.188952642203366e-06, 8.086529148791978e-06, 8.031416427413482e-06, 7.941499220214354e-06, 7.893213050615638e-06]\n",
      "23/08/14 17:36:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/08/14 17:36:27 WARN DAGScheduler: Broadcasting large task binary with size 1409.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 1877:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| 1744.0112953690405|\n",
      "| 3673.0776926113685|\n",
      "|  25372.50067537486|\n",
      "| 27616.518169920164|\n",
      "|-26372.467521783197|\n",
      "|-147.04842934495537|\n",
      "|  1717.156290305953|\n",
      "|    22953.034057051|\n",
      "| -17444.16716856684|\n",
      "| -38647.59101147298|\n",
      "|  -5898.21343120758|\n",
      "| -18060.61832763377|\n",
      "| -6788.469839933154|\n",
      "|-3590.8500175218796|\n",
      "| -22021.66416239168|\n",
      "|-13261.064974018838|\n",
      "|  11873.30507538229|\n",
      "| -3150.849411241128|\n",
      "|  37005.56945533812|\n",
      "|-30731.531878367008|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 22808.906513\n",
      "r2: 0.999984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(labelCol=TARGET_COL, maxIter=100, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel18 = lr.fit(dataset)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel18.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel18.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "training18Summary = lrModel18.summary\n",
    "print(\"numIterations: %d\" % training18Summary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(training18Summary.objectiveHistory))\n",
    "training18Summary.residuals.show()\n",
    "print(\"RMSE: %f\" % training18Summary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % training18Summary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "crDSEqZzK_xs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 1881:>                                                       (0 + 1) / 1]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_path = \"reg_model\"\n",
    "lrModel18.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel: uid=LinearRegression_0c927c19f4ac, numFeatures=32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel: uid=LinearRegression_0c927c19f4ac, numFeatures=32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "loaded_model = LinearRegressionModel.load(model_path)\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================== Consumer Part =================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'bootstrap.servers': \"localhost:9092\",\n",
    "        'group.id': \"foo\",\n",
    "        'auto.offset.reset': 'smallest'}\n",
    "\n",
    "consumer = Consumer(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "running = True\n",
    "def kafka_consumer(consumer):\n",
    "    output_path = \"\"\n",
    "    try:\n",
    "        if not os.path.exists(output_folder):\n",
    "            # Create the directory\n",
    "            os.makedirs(output_folder)\n",
    "            print(f\"Directory '{output_folder}' created.\")\n",
    "        else:\n",
    "            print(f\"Directory '{output_folder}' already exists.\")\n",
    "        consumer.subscribe(topic)\n",
    "        while running:\n",
    "            msg = consumer.poll(timeout=3.0)\n",
    "            if msg is None:\n",
    "                break\n",
    "                \n",
    "            if msg.error():\n",
    "                if msg.error():\n",
    "                    raise Exception(msg.error())\n",
    "                    \n",
    "            print(\"starting prediction for \", msg.value().decode('utf-8'))\n",
    "            prediction = runModel(msg.value().decode('utf-8'))\n",
    "            if prediction is not None:\n",
    "                prediction.show()\n",
    "                output_path = output_folder + '/' + msg.value().decode('utf-8') + '.txt'\n",
    "                writeToFile(output_path, prediction)\n",
    "\n",
    "            else:\n",
    "                print(\"The player is not valid for this time period.\")\n",
    "\n",
    "\n",
    "\n",
    "    finally:\n",
    "        # Close down consumer to commit final offsets.\n",
    "        print('Finished task')\n",
    "        consumer.close()\n",
    "\n",
    "def shutdown():\n",
    "    running = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(name):\n",
    "    data = checkIfValidName(name)\n",
    "    print(data)\n",
    "    if data is not None:\n",
    "        return loaded_model.transform(data)\n",
    "    else:\n",
    "        print(\"Error, the moesdel was not trained on the specified player\")\n",
    "\n",
    "\n",
    "# Make sure we have previous resutlts for that player\n",
    "def checkIfValidName(name):\n",
    "    url = 'https://gist.githubusercontent.com/Aviad-Hedvat/4de2aa4fa83a418fe4ab2ea3995ded36/raw/d8ec5823d09bc040c1e9f4988d7ad75202c4e30c/dataset.csv'\n",
    "    spark.sparkContext.addFile(url)\n",
    "    df = spark.read.csv(\"file://\"+SparkFiles.get(\"dataset.csv\"), header=True, inferSchema= True)\n",
    "    tmp = df.filter(f'Name == \"{name}\"')\n",
    "    if tmp.count() > 0:\n",
    "        features = tmp.columns\n",
    "        features.remove('Name')\n",
    "        vectorAssembler = VectorAssembler(inputCols = features, outputCol = 'features')\n",
    "        vdf = vectorAssembler.transform(tmp)\n",
    "        vdf = vdf.select('features')\n",
    "        return vdf\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "    \n",
    "def writeToFile(output_path, prediction):\n",
    "    try:\n",
    "        with open(output_path, 'w') as file:\n",
    "            file.write(str(prediction.select('prediction').collect()[0][0]))\n",
    "        print(f\"Prediction written to '{output_path}' successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "def defineSchema():\n",
    "    return StructType([\n",
    "        StructField('features', VectorUDT(), True),\n",
    "        StructField('ANNUAL_GROSS', IntegerType(), True)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'Predictions' already exists.\n",
      "starting prediction for  Adam Ounas\n",
      "23/08/14 18:16:51 WARN SparkContext: The path https://gist.githubusercontent.com/Aviad-Hedvat/4de2aa4fa83a418fe4ab2ea3995ded36/raw/d8ec5823d09bc040c1e9f4988d7ad75202c4e30c/dataset.csv has been added already. Overwriting of added paths is not supported in the current version.\n",
      "1\n",
      "DataFrame[features: vector]\n",
      "+--------------------+-----------------+\n",
      "|            features|       prediction|\n",
      "+--------------------+-----------------+\n",
      "|[0.273,0.309,0.28...|2950224.501814125|\n",
      "+--------------------+-----------------+\n",
      "\n",
      "Prediction written to 'Predictions/Adam Ounas.txt' successfully.\n",
      "Finished task\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName(\"FIFA_Consumer\")\\\n",
    "        .getOrCreate()\n",
    "output_folder = \"Predictions\"\n",
    "model_path = \"reg_model\"\n",
    "topic = [\"prediction\"]\n",
    "kafka_consumer(consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
